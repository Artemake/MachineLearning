{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning - Linear Models #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent for Multiple Variables ##\n",
    "\n",
    "The gradient descent equation itself is generally the same form; we just have to repeat it for our 'n' features:\n",
    "\n",
    "$\\begin{align*} & \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_0^{(i)}\\newline \\; & \\theta_1 := \\theta_1 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_1^{(i)} \\newline \\; & \\theta_2 := \\theta_2 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_2^{(i)} \\newline & \\cdots \\newline \\rbrace \\end{align*}$\n",
    "\n",
    "The following image compares gradient descent with one variable to gradient descent with multiple variables:\n",
    "![](gradient_multiple_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try this out ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of diabetes dataset: \n",
      "dict_keys(['data', 'target', 'DESCR', 'feature_names', 'data_filename', 'target_filename'])\n",
      ".. _diabetes_dataset:\n",
      "\n",
      "Diabetes dataset\n",
      "----------------\n",
      "\n",
      "Ten baseline variables, age, sex, body mass index, average blood\n",
      "pressure, and six blood serum measurements were obtained for each of n =\n",
      "442 diabetes patients, as well as the response of interest, a\n",
      "quantitative measure of disease progression one year after baseline.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "  :Number of Instances: 442\n",
      "\n",
      "  :Number of Attributes: First 10 columns are numeric predictive values\n",
      "\n",
      "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
      "\n",
      "  :Attribute Information:\n",
      "      - Age\n",
      "      - Sex\n",
      "      - Body mass index\n",
      "      - Average blood pressure\n",
      "      - S1\n",
      "      - S2\n",
      "      - S3\n",
      "      - S4\n",
      "      - S5\n",
      "      - S6\n",
      "\n",
      "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n",
      "\n",
      "\n",
      "...\n",
      "['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "# Lets explore our data\n",
    "print(\"Keys of diabetes dataset: \\n{}\".format(diabetes.keys()))\n",
    "\n",
    "# Description of the dataset \n",
    "print(diabetes['DESCR'][:900] + \"\\n...\")\n",
    "\n",
    "print(diabetes['feature_names'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>437</td>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.059744</td>\n",
       "      <td>-0.005697</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.007207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>-0.005515</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>-0.067642</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>0.079165</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>-0.018118</td>\n",
       "      <td>0.044485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>439</td>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>0.017282</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.013840</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.046879</td>\n",
       "      <td>0.015491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.016318</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.044528</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441</td>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.073030</td>\n",
       "      <td>-0.081414</td>\n",
       "      <td>0.083740</td>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.173816</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>0.003064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2    0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
       "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
       "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
       "439  0.041708  0.050680 -0.015906  0.017282 -0.037344 -0.013840 -0.024993   \n",
       "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
       "441 -0.045472 -0.044642 -0.073030 -0.081414  0.083740  0.027809  0.173816   \n",
       "\n",
       "            7         8         9  \n",
       "0   -0.002592  0.019908 -0.017646  \n",
       "1   -0.039493 -0.068330 -0.092204  \n",
       "2   -0.002592  0.002864 -0.025930  \n",
       "3    0.034309  0.022692 -0.009362  \n",
       "4   -0.002592 -0.031991 -0.046641  \n",
       "..        ...       ...       ...  \n",
       "437 -0.002592  0.031193  0.007207  \n",
       "438  0.034309 -0.018118  0.044485  \n",
       "439 -0.011080 -0.046879  0.015491  \n",
       "440  0.026560  0.044528 -0.025930  \n",
       "441 -0.039493 -0.004220  0.003064  \n",
       "\n",
       "[442 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "diabetes_X = np.array(diabetes.data)\n",
    "display(pd.DataFrame(diabetes_X))\n",
    "\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-20] #80%\n",
    "diabetes_X_test = diabetes_X[-20:] #20%\n",
    "# print(diabetes_X_train)\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = diabetes.target[:-20]\n",
    "diabetes_y_test = diabetes.target[-20:]\n",
    "# print(diabetes_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "diabetes_y_pred = regr.predict(diabetes_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [ 3.03499549e-01 -2.37639315e+02  5.10530605e+02  3.27736980e+02\n",
      " -8.14131709e+02  4.92814588e+02  1.02848452e+02  1.84606489e+02\n",
      "  7.43519617e+02  7.60951722e+01]\n",
      "Mean squared error: 2004.57\n",
      "Variance score: 0.59\n"
     ]
    }
   ],
   "source": [
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent in Practice I - Feature Scaling ##\n",
    "\n",
    "We can speed up gradient descent by having each of our input values in roughly the same range. This is because Î¸ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\n",
    "\n",
    "The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. Ideally:\n",
    "\n",
    "$$âˆ’1 â‰¤ x_{(i)} â‰¤ 1$$ or $$âˆ’0.5 â‰¤ x_{(i)}â‰¤ 0.5 $$\n",
    "\n",
    "These aren't exact requirements; we are only trying to speed things up. The goal is to get all input variables into roughly one of these ranges, give or take a few.\n",
    "\n",
    "Two techniques to help with this are **feature scaling** and mean **normalization**. Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown in this formula:\n",
    "\n",
    "$$x_i := \\frac{x_i - Î¼_i}{s_i} $$\n",
    "\n",
    "Where $Î¼_i$ is the average of all the values for feature (i) and $s_i$ is the range of values (max - min), or $s_i$ is the standard deviation.\n",
    "\n",
    "For example, if $x_i$ represents housing prices with a range of 100 to 2000 and a mean value of 1000, then, $xi:= \\dfrac{price-1000}{1900}$ .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning - Logistic Regression #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like a Linear Regression model, a Logistic Regression model computes a weighted sum of the input features (plus a bias term), but instead of outputting the result directly like the Linear Regression model does, it outputs the logistic of this result.\n",
    "\n",
    "$$\n",
    "\\hat{p} = h_{\\boldsymbol{\\theta}}(\\mathbf{x}) = \\sigma(\\boldsymbol{\\theta}^T \\mathbf{x})\n",
    "$$\n",
    "\n",
    "The logisticâ€”also called the logit, noted Ïƒ(Â·)â€”is a sigmoid function (i.e., S-shaped) that outputs a number between 0 and 1.\n",
    "\n",
    "$$\n",
    "\\sigma(t) = \\dfrac{1}{1 + \\exp(-t)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import section\n",
    "from matplotlib import pylab\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "#sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "mySamples = []\n",
    "mySigmoid = []\n",
    "\n",
    "# generate an Array with value ???\n",
    "# linespace generate an array from start and stop value\n",
    "# with requested number of elements. Example 10 elements or 100 elements.\n",
    "# \n",
    "y = plt.linspace(-10,10,1000)\n",
    "\n",
    "# prepare the plot, associate the color r(ed) or b(lue) and the label \n",
    "plt.plot(y, sigmoid(y), 'b', label='linspace(-10,10,100)')\n",
    "\n",
    "# Draw the grid line in background.\n",
    "plt.grid()\n",
    "\n",
    "# Title & Subtitle\n",
    "plt.title('Sigmoid Function')\n",
    "plt.suptitle('Sigmoid')\n",
    "\n",
    "# place the legen boc in bottom right of the graph\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# write the Sigmoid formula\n",
    "plt.text(4, 0.8, r'$\\sigma(x)=\\frac{1}{1+e^{-x}}$', fontsize=15)\n",
    "\n",
    "#resize the X and Y axes\n",
    "plt.gca().xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "plt.gca().yaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    " \n",
    "\n",
    "# plt.plot(x)\n",
    "plt.xlabel('X Axis')\n",
    "plt.ylabel('Y Axis')\n",
    "\n",
    "# create the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the Logistic Regression model has estimated the probability $p = h_{\\theta}(x)$ that an instance $x$ belongs to the positive class, it can make its prediction $\\hat{y}$ easily. \n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases}\n",
    "  0 & \\text{if } \\hat{p} < 0.5, \\\\\n",
    "  1 & \\text{if } \\hat{p} \\geq 0.5.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Notice that $\\sigma(t) < 0.5$ when $t < 0$, and $\\sigma(t) â‰¥ 0.5$ when $t â‰¥ 0$, so a Logistic Regression\n",
    "model predicts 1 if $Î¸^T Â· x$ is positive, and 0 if it is negative.\n",
    "\n",
    "The objective of training is to set the parameter vector $\\theta$ so that the model estimates high probabilities for positive instances $(y = 1)$ and low probabilities for negative instances $(y = 0)$.\n",
    "\n",
    "Cost function of single example:\n",
    "\n",
    "$$\n",
    "c(\\boldsymbol{\\theta}) =\n",
    "\\begin{cases}\n",
    "  -\\log(\\hat{p}) & \\text{if } y = 1, \\\\\n",
    "  -\\log(1 - \\hat{p}) & \\text{if } y = 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "This cost function makes sense because $â€“ log(t)$ grows very large when t approaches 0, so the cost will be large if the model estimates a probability close to 0 for a positive instance, and it will also be very large if the model estimates a probability close to 1 for a negative instance. On the other hand, â€“ log(t) is close to 0 when t is close to 1, so the cost will be close to 0 if the estimated probability is close to 0 for a negative instance or close to 1 for a positive instance, which is precisely what we want.\n",
    "\n",
    "The cost function over the whole training set is simply the average cost over all training instances. It can be written in a single expression (as you can verify easily), called the log loss.\n",
    "\n",
    "$$\n",
    "J(\\boldsymbol{\\theta}) = -\\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{\\left[ y^{(i)} log\\left(\\hat{p}^{(i)}\\right) + (1 - y^{(i)}) log\\left(1 - \\hat{p}^{(i)}\\right)\\right]}\n",
    "$$\n",
    "\n",
    "\n",
    "This cost function is convex, so Gradient Descent (or any other optimization algorithm) is guaranteed to find the global minimum (if the learning rate is not too large and you wait long enough). The partial derivatives of the cost\n",
    "function with regards to the $j$th model parameter $\\theta_j$ is given by\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial}{\\partial \\theta_j} \\text{J}(\\boldsymbol{\\theta}) = \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\left(\\mathbf{\\sigma(\\boldsymbol{\\theta}}^T \\mathbf{x}^{(i)}) - y^{(i)}\\right)\\, x_j^{(i)}\n",
    "$$\n",
    "\n",
    "\n",
    "The Logistic Regression model can be generalized to support multiple classes directly, without having to train and combine multiple binary classifiers. This is called Softmax Regression, or Multinomial Logistic Regression.\n",
    "\n",
    "\n",
    "The idea is quite simple: when given an instance x, the Softmax Regression model first computes a score sk(x) for each class $k$, then estimates the probability of each class by applying the softmax function (also called the normalized exponential) to the scores. The equation to compute $s_k(x)$ should look familiar, as it is just like the equation\n",
    "for Linear Regression prediction. \n",
    "\n",
    "$$\n",
    "s_k(\\mathbf{x}) = ({\\boldsymbol{\\theta}^{(k)}})^T \\mathbf{x}\n",
    "$$\n",
    "\n",
    "\n",
    "Note that each class has its own dedicated parameter vector $Î¸_k$. All these vectors are\n",
    "typically stored as rows in a parameter matrix $\\Theta$.\n",
    "\n",
    "Once you have computed the score of every class for the instance x, you can estimate the probability $\\hat{p}_k$ that the instance belongs to class k by running the scores through the softmax function. it computes the exponential of every score, then normalizes them (dividing by the sum of all the exponentials).\n",
    "\n",
    "$$\n",
    "\\hat{p}_k = \\sigma\\left(\\mathbf{s}(\\mathbf{x})\\right)_k = \\dfrac{\\exp\\left(s_k(\\mathbf{x})\\right)}{\\sum\\limits_{j=1}^{K}{\\exp\\left(s_j(\\mathbf{x})\\right)}}\n",
    "$$\n",
    "\n",
    "- $K$ is the number of classes.\n",
    "- $s(x)$ is a vector containing the scores of each class for the instance x.\n",
    "- $Ïƒ(s(x))_k$ is the estimated probability that the instance x belongs to class k given\n",
    "the scores of each class for that instance.\n",
    "\n",
    "Just like the Logistic Regression classifier, the Softmax Regression classifier predicts the class with the highest estimated probability (which is simply the class with the highest score).\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\underset{k}{\\operatorname{argmax}} \\, \\sigma\\left(\\mathbf{s}(\\mathbf{x})\\right)_k = \\underset{k}{\\operatorname{argmax}} \\, s_k(\\mathbf{x}) = \\underset{k}{\\operatorname{argmax}} \\, \\left( ({\\boldsymbol{\\theta}^{(k)}})^T \\mathbf{x} \\right)\n",
    "$$\n",
    "\n",
    "- The argmax operator returns the value of a variable that maximizes a function. In this equation, it returns the value of k that maximizes the estimated probability \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
